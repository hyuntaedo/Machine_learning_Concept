{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP5TY8N/eE8Lo7SUd+YC7yC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyuntaedo/Machine_learning_Concept/blob/main/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4l3EnWQh5UH"
      },
      "source": [
        "#<strong>XGBoost</strong>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNSzNl87h9rz"
      },
      "source": [
        "- 요즘 가장 유명한 ML알고리즘 중 하나이다.\n",
        "- XGBoost는 다른 ML알고리즘보다 더 나은 솔루션을 제공하는것으로 알려져 있다.\n",
        "- XGBoost의 inception때문에 구조적 데이터를 다루기 위한 최신 ML알고리즘이 되었다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdkEhLdDmKqB"
      },
      "source": [
        "# 무엇이 XGBoost를 유명하게 하였는가?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erl0TrEkmRJ8"
      },
      "source": [
        "- 속도와 성능 : 원래 C++로 작성되어있어, 앙상블 분류기에 비해 빠르다\n",
        "- 병렬코어 알고리즘 : 코어 XGBoost 알고리즘이 병렬이기 때문에, 멀티코어 프로세스를 활요할 수 있다. 또한 매우 대규모 데이터 셋에서 훈련이 가능하도록 만드는 GPU와 Computer Network와도 병렬이다.\n",
        "- 지속적응로 다른 알고리즘보다 좋은 성능 : 다양한 ML벤치마크에서 더 나은 성능을 보인다.\n",
        "- 다양한 튜닝 파라미터 : 교차검증(Cross-validation), 정규화(Normalization), 사용자 정의 목적함수(user-define object function), 손실값(missing vlaue), 트리 매개변수(tree parameter), 호환가능한 API등에대한 파라미터를 갖는다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYo5AG-7m1X3"
      },
      "source": [
        "# Boosting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-el0qR5Tm5zJ"
      },
      "source": [
        "- 부스팅은 앙상블이론에 따라 동작하는 순차적(sequential)기술\n",
        "- 부스팅은 약 학습자(seak learner)를 결합하고, 개선된 예측 정확도를 전달한아 임의의 순산 t에서 모델결과는 이전 순간 t-1의 결과를 기초로 가중된다 올바르게 예측된 결과는, 더 낮은 가중치가 주어지고, 잘못 분류된 것은 더 높은 가중치가 부여된다.\n",
        "- 약 학습기는 임의로 추측하는 것보다 약간 더 좋다. 예측이 50%보다 약간 더 좋은 결정트리를 예로 들 수 있다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq3Y-SxrqWxx"
      },
      "source": [
        "# XBoost's Hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAbn3SlhqvFO"
      },
      "source": [
        "- learning_rate : 오버피팅을 막기 위해 사용되는 축소계 크기로 0과 1사이 값이다.\n",
        "- max_depth : 부스팅 라운드동안 각각의 트리가 얼마나 깊게 성장할 수 있는지 결정\n",
        "- subsample : 트리당 사용되는 샘플의 비율. 낮은 값은 언더피팅으로 이어질 수 있다.\n",
        "- colsample_bytree : 트리당 사용되는 특성의 비율, 높은 값은 오버피팅으로 이어질 수 있다.\n",
        "- objective : 손실함수 결정\n",
        "  - reg:linear : 회귀문제\n",
        "  - reg:logistic : 오직 결정(선택) 만 있는 문제\n",
        "  - binary:logistic : 확률을 포함하는 분류 문제\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9fiN8wTrNeD"
      },
      "source": [
        "- XGBoost는 더 복잡해진 모델을 제한하기위해 정규화 파라미터를 지원하여 더 간단한 모델로 감소시킨다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IlV9CJ-rSmd"
      },
      "source": [
        "  - gamma : 분할 후 손실에서 예상 감소에 기초하여 주어진 노드를 분할할지 여부를 제어한다. 높은 값은 더 적은 분할로 이어지고, 오직 트리기반 학습만 지원된다.\n",
        "  - alpah : 리프(leaf)가중치에 L1정규화, 큰값은 더 많은 정규화로 이어진다.\n",
        "  - lambda : 리프가중치에 L2정규화를 하고 L1정규화 보다 더 부드럽다.(Smooth)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q5TN6j3rhxP"
      },
      "source": [
        "# K-Fold Cross Validation using XGBoost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Lu_8upspd0"
      },
      "source": [
        "- 좀 더 가력한 모델을 구축하기 위해서는 보통 원본 훈련 데이터셋의 모든 항목이 훈련과 검증 모두에 사용되는 k-flod 교차검증을 수행한다. 또한 각 항목은 단 한번만 검증에 사용된다. XGBoost는 cv() 메소드를 통해 k-fold 교차검증을 지원한다. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3fmkktes3KO"
      },
      "source": [
        "## parameter\n",
        "- num_boost_round : 구축한 트리의 수를 나타낸다.(n_etsimater)와 비슷\n",
        "- metrics : 교차검증동안 관찰할 평가 지표\n",
        "- as_pandas : 판다스 데이터프레임으로 결과 반환\n",
        "- early_stopping_rounds : 만약 주어진 횟수 동안 정해진 표가 개선되지 않으면, 일찍 모델의 훈련을 종료\n",
        "- seed : 결과의 재현성을 위함\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJFEFY-ttMmk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-E-uXKbqhf2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK324BLsmd6D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}